# 2. 深層学習のための最適化 (Optimization)

### 問1

次の計算グラフは、ロジット \( z = [z_1, z_2, z_3] \) からソフトマックスを計算し、クロスエントロピー損失 \( L \) を求める順伝播を表している。

```{mermaid}
graph LR
    Z1[z1] ---> S[softmax]
    Z2[z2] ---> S[softmax]
    Z3[z3] ---> S[softmax]
    S ---> Yhat1[ŷ1]
    S ---> Yhat2[ŷ2]
    S ---> Yhat3[ŷ3]
    Y1[y1=0] ---> CE[Cross-Entropy Loss L]
    Y2[y2=1] ---> CE
    Y3[y3=0] ---> CE
    Yhat1 ---> CE
    Yhat2 ---> CE
    Yhat3 ---> CE
```

I. 逆伝播で$\frac{∂𝐿}{∂𝑧_j}$を求めたときの一般公式として正しいものを選べ。  
ただし、ソフトマックスとクロスエントロピー損失は以下で表せるものとする。  

$$
\hat{y}_i = \frac{\exp(z_i)}{\sum_{k} \exp(z_k)}
$$

$$
L = - \sum_{i} y_i \log(\hat{y}_i)
$$

A. $𝑦_j−\hat{y}_j$  
B. $\hat{y}_j-𝑦_j$  
C. $\frac{-y_j}{\hat{y}_j}$  
D. $\hat{y}_j(1-𝑦_j)$

II.3クラス分類問題で、真のラベル$y = [0,1,0]$、ソフトマックスの出力$\hat{y}=[0.2, 0.3, 0.5]$のとき、
$\frac{∂𝐿}{∂𝑧_2}$を求めよ。  
A. -0.7  
B. 0.7  
C. -0.3  
D. 0.3

### 問2

AdaGradアルゴリズムの数式は以下のように表される：

$$
h_t = h_{t-1} + ∇E(W_t) ⊙ ∇E(W_t)
α_t = α_0 × (1/√(h_t + ε))
W_{t+1} = W_t - α_t ⊙ ∇E(W_t)
$$

この数式に関する説明として正しくないものはどれか。

A. h_tは各パラメータの累積二乗勾配を表し、そのパラメータがどれだけ激しく更新されてきたかを記録する  
B. α_tは各パラメータごとに適応的に調整される学習率で、累積二乗勾配が大きいほど学習率は小さくなる  
C. ⊙はアダマール積（要素ごとの積）を表し、各パラメータを独立して扱うために使用される  
D. εは数値安定化項で、h_tが大きくなりすぎて学習率が極端に小さくなることを防ぐ